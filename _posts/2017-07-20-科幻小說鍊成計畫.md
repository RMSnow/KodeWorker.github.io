---
layout: single
title: 科幻小說鍊成計畫(構建中)
date: 2017-07-20 12:59:51
excerpt: 紀錄我用程式自動生成科幻小說的學習歷程。
categories:
- 計畫
tags:
- procedural generation
- NaNoWriMo
- NaNoGenMo
- web scraping
- LSTM
- GAN
---

{% include toc title = "目錄" %}

# 前言

十一月越來越逼進了，那是一年一度的[國家小說寫作月](http://nanowrimo.org/)(National Novel Writing Month)了，身為文藝愛好者的我，怎麼可能錯過這場盛事呢？
雖然還有好一段時間，但是現在不好好開始規劃，慢慢利用零碎的時間持之以恆的學習，應該很難產生出有趣的成果。
在年初時候，我對遊戲相關的演算法非常感興趣，當時實作了尋找路徑的A*演算法、以及取得視野的ray-casting，接著慢慢地把興趣轉向了程序化生成(procedural generation)。
在網路上找到了一篇滿有趣的[文章](https://mewo2.com/notes/naming-language/)，作者是由奇幻小說的地名為發想，想要創造一組虛構的語言來對小說中的世界進行命名，文章中一步步從語言學的角度，由音節、字符逐步構建簡單的語言。
仔細看了一下文章開頭，天那！竟然是為了[NaNoGenMo](https://nanogenmo.github.io/)，國家小說產生月！
在寒冬時節，想像屋外冰天雪地，愜意地坐在火爐邊寫小說、讀小說，外加一杯香濃的熱可可，真是人生一大享受。
但是這時一群腦洞大開的程式員，不畏寒冬守在螢幕前敲著鍵盤，打算用程式碼來拯救全世界。

>「你用筆來寫小說？」
>
>「太遜了，我們都用程式來生成小說」

引起我極大興趣的另一個原因是，去年年底在reddit上讀到了關於Machine Learning conference的報導，其中Generative Adversarial Networks(GAN)受到高度的評價，被譽為是近十年來最棒的演算法。
最近台大[李宏毅教授](http://speech.ee.ntu.edu.tw/~tlkagk/index.html)也來到中心演講也是在介紹GAN，其中就有唐詩鍊成的範例，雖然語句不太有意義，但我認為是一個很好的參考指標。
接著是清大[張俊盛教授](http://www.nlplab.cc/)來中心介紹自然語言處理，這個課程講得概念很粗略，反而是老師為自然語言、機器翻譯畫了一個美好大餅，希望吸引博士生加入一起努力(讀博士？算了吧！)。
不過重點有提到，目前主流使用的Neural Machine Translation，由於忽略的語句的文法構成，仍有許多值得改進的地方。
聽完這一系列的演講，內心的熱血已在蠢蠢欲動，所以想利用NaNoGenMo這個性質比較輕鬆的活動，慢慢從頭規劃程式並產生至少能讀的科幻小說。

# 程式規劃
在施放鍊金術前，總是要有基本素材才能進行鍊成，直接無中生有已經不是鍊金術了，那已經是另一個次元位面的事情了。
大致上先將目前計畫規劃為下列三大部分，再逐一慢慢完成每個區塊。

1. 網路爬蟲：收集已在公共領域、不會侵權的小說文章。
2. 文檔正規化
3. 構建文句產生器：參考[這篇文章](http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)，利用LSTM自動產生文句。
4. 利用GAN鍊出最佳文章

先建立一個[Github Project](https://github.com/KodeWorker/NovelAlchemist)來存放我的工作進度。

## 網路爬蟲
### 網站資源
為了要生成科幻小說，想當然耳要先去找尋相關類型的文章，有礙於著作權法，所以只能找尋已經開放於公共領域(public domain)的小說。
在網路上搜索相關資訊的過程中，發現了一篇介紹[25個存放公共領域書籍的免費網站](https://ebookfriendly.com/free-public-domain-books-sources/)，裡面有很完整的介紹。
其中最大的網站，[Project Gutenberg](http://www.gutenberg.org/)，是我心目中收集鍊金素材的最佳網站，但是很不幸它的使用條款其中一條寫道：

> **The Project Gutenberg website is for human users only.** Any real or perceived use of automated tools to access our site will result in a block of your IP address. This site utilizes cookies, captchas and related technologies to help assure the site is maximally available for human users only.

雖然之前看的[書本](https://automatetheboringstuff.com/chapter11/)裡，介紹到Web Scraping也是從Project Gutenberg下載羅密歐與茱麗葉，但是身為腦包研究者一定先把25個網站一個個看過，再決定我們的行動方針。
在排名第6的[Feedbooks](http://www.feedbooks.com/publicdomain)也是有非常多的公共領域書籍，其中科幻類的就有1176本！但是看到使用條例，心也涼了一半，不過條款的力道似乎沒有像Project Gutenberg那麼嚴厲。

> **6.15** use any robot, spider, scraper, or other automated means to access the FeedBooks Website bypass any measures FeedBooks may use to prevent or restrict access to the FeedBooks Website

小王子曾說過：『沙漠之所以這麼美，是因為在某個角落裡，藏著一口井。星星好美，是因為一朵我們看不見的花。』
現在終於讓我找到了，那個藏在沙漠角落的那一口井了，那就是[Manybooks](http://manybooks.net/)，這個網站雖然也收集了Project Gutenberg及其他網路資源，但是使用條款中完全沒有限制網路爬蟲，簡直是佛心公司。

### 程式撰寫
我們會用到以下packges
{% highlight python %}
    import os
    import time
    import random
    import bs4
    import requests
{% endhighlight %}

整體的主程式架構如下，概念是先取得網站上所有的小說類別，接著再搜尋相關語言選項。
由於個人就是偏好科幻小說，而這個計畫也叫做"科幻"小說鍊成計畫，當然挑選Science Fiction。
在語言選擇方面，由於網路上大部分的資料是英語，為了方便資料處理與模型建置，首選English。
接著一頁一頁的爬過所有科幻類的頁面，每一頁開始尋找所有書籍的連結，並將所有書籍的`.epub`檔案下載(因為有些書籍只有`.epub`檔案)。
最後搜尋下個頁面的按鈕，若無按鈕則結束程式。
{% highlight python %}
    if __name__ == '__main__':

        # Settings
        file_dir = os.path.join(os.path.dirname(__file__),'epub')
        url = 'manybooks.net'

        # Create the folder to save scraped files
        if not os.path.exists(file_dir):
            os.makedirs(file_dir)

        # select=54 : Science Fiction
        genre_link = sel_genre(url, select=54)
        # select=10 : English
        lang = sel_language(genre_link, select=10)
        # First page of the selected genre
        page_link = genre_link + '/' + lang

        while True:

            # Access genre pages
            print('access... %s' %page_link)
            res = requests.get(page_link)
            try:
                res.raise_for_status()
            except Exception as exc:
                print('There was a problem accessing genre page: %s' %(exc))
            soup = bs4.BeautifulSoup(res.text)

            try:
                # Find books and download .epub files
                download_books_in_page(url, soup, file_dir)

                # Get "next page" button
                button_elems = soup.select('a[title="next"]')[0]
                page_link = 'http://%s%s' %(url, button_elems.get('href'))
            except Exception as e:
                # No "next page" button
                print(e)
                print('end of pages')
                break
{% endhighlight %}

首先我們要先取得所有小說的類型，如此一來方便我們未來的使用，可以挑選其他非科幻題材來鍊成文章。
{% highlight python %}
    def sel_genre(url='manybooks.net', select=None):
        """ Select Genre
        This function allows user to select a particular genre from "manybooks.net"
        and it returns the link to the first page of selected genre.

        Parameters
        ----------
        url: string, default='manybooks.net'
            The URL of "Manybooks".
        select: int, default=None
            The number of selection. If the value is None, the console will display
            all the genres and requires an user input.

        Return
        ------
        link: string
            The URL of the first page of the selected genre.
        """
        genre_url = 'http://%s/%s' %(url, 'categories')
        genre_dict = {}
        genre_list = []

        # Access genre catagories
        print('access genres... %s' %genre_url)
        res = requests.get(genre_url)
        try:
            res.raise_for_status()
        except Exception as exc:
            print('There was a problem accessing genre catagories: %s' %(exc))
        soup = bs4.BeautifulSoup(res.text)

        # Get all genre elements
        elems = soup.select('a[class="larger"]')
        for elem in elems:
            genre_name = elem.getText()
            genre_link = 'http://%s%s' %(url, elem.get('href'))
            genre_dict[genre_name] = genre_link
            genre_list.append(genre_name)

        # Display Genre Selection
        if select == None:
            for i in range(len(genre_list)):
                print('[%d] : %s' %(i+1, genre_list[i]))
            select = input('Select the Genre (no.) >> ')

            if not (int(select) > 0 and int(select) < len(genre_list)+1):
                raise ValueError('Wrong selection number!')

        print('genre: %s' %genre_list[int(select)-1])

        link = genre_dict[genre_list[int(select)-1]]
        return link
{% endhighlight %}

接著要選取對應的語言。
{% highlight python %}
    def sel_language(url, select=None):
        """ Select Language
        This function allows user to select a particular language in the pages of
        each genre and it returns the link to the page of books with selected
        language.

        Parameters
        ----------
        url: string
            The URL of the first selected genre page.
        select: int, default=None
            The number of selection. If the value is None, the console will display
            all the language and requires an user input.

        Return
        ------
        link: string
            The URL of the first page of the selected genre and selected language.
        """
        lang_dict = {}
        lang_list = []

        # Access language options
        print('access language... %s' %url)
        res = requests.get(url)
        try:
            res.raise_for_status()
        except Exception as exc:
            print('There was a problem accessing language options: %s' %(exc))
        soup = bs4.BeautifulSoup(res.text)

        # Get all language option elements
        elems = soup.select('option')
        for elem in elems:
            lang_name = elem.getText()
            lang_value = elem.get('value')
            lang_dict[lang_name] = lang_value
            lang_list.append(lang_name)

        # Display language Selection
        if select == None:
            for i in range(len(lang_list)):
                print('[%d] : %s' %(i+1, lang_list[i]))
            select = input('Select the Genre (no.) >> ')

            if not (int(select) > 0 and int(select) < len(lang_list)+1):
                raise ValueError('Wrong selection number!')

        print('language: %s' %lang_list[int(select)-1])
        return lang_dict[lang_list[int(select)- 1]]
{% endhighlight %}

最後就是最重要，下載頁面的部份。首先是取得一整個頁面所有書籍的連結，每一頁約略有20本書。
{% highlight python %}
    def download_books_in_page(url, soup, file_dir):
        """Download Books in the Genre Pages
        This function downloads all the books in a genre page and raise the stop
        signal if there is no books in the page.

        Parameters
        ----------
        url: string
            The root URL address.
        soup: bs4.BeautifulSoup object
            The object contains all the info. in the genre catagory page.
        file_dir: sting
            This is the folder to save the scraped files.
        """
        try:
            # has books
            # Get book elements in the genre page
            book_elems = soup.select('.grid_6.smallBook a')
            book_links =  [elem.get('href') for elem in book_elems]
            book_names = [elem.getText() for elem in book_elems]
            for i in range(len(book_links)):
                # Download one book at a time
                download_single_book(url, book_links[i], file_dir, book_names[i])
        except:
            # no book
            raise ScraperError('Error: No books in this page!')
{% endhighlight %}
類別頁面沒有書的時候，發出的以下自定義的Error。
{% highlight python %}
    class ScraperError(Exception):
        """ Scraper Error
        A custom error raised when there are troubles.
        """
        def __init__(self, value):
            self.value = value
        def __str__(self):
            return repr(self.value)
{% endhighlight %}
最最最關鍵，下載書籍連結的部分如下。
{% highlight python %}
    def download_single_book(url, book_link, file_dir, book_name):
        """Download a single Book
        This function download the particular book according to its book_link. We
        use URL (see -> # Text file download link) instead of using selenium module
        for better program control.

        Parameters
        ----------
        url: string
            The root URL address.
        book_link: string
            The href of the book.
        file_dir: string
            This is the folder to save the scraped files.
        book_name
            This is the name(filename) of the book.
        """
        # Text file download link
        book_id = book_link[8:-5]
        download_page = 'http://' + url + '/download-ebook?tid=' + book_id + \
        '&book=1%3Aepub%3A.epub%3Aepub'

        # Access the download page
        print('-> access... %s' %(book_name))
        res = requests.get(download_page)
        try:
            res.raise_for_status()
        except Exception as exc:
            print('There was a problem accessing download page: %s' %(exc))
        soup = bs4.BeautifulSoup(res.text)

        # Get downloadable elements
        download_elems = soup.select('a[rel="nofollow"]')
        epub_link = [elem.get('href') for elem in download_elems if \
                    elem.get('href').endswith('.epub')][0]

        # Download .epub file
        download_link = 'http://%s%s' %(url, epub_link)
        print('->-> access download link... %s' %(download_link))
        res = requests.get(download_link, allow_redirects=True)  # redirected!
        try:
            res.raise_for_status()
        except Exception as exc:
            print('There was a problem accessing download link: %s' %(exc))
        print('->-> download... %s' %(book_name))

        # Avoid file name errors
        book_name = book_name.replace('/', '-')
        book_name = book_name.replace('\\', '-')
        book_name = book_name.replace(':', '-')
        book_name = book_name.replace('*', '-')
        book_name = book_name.replace('?', '-')
        book_name = book_name.replace('\"', '-')
        book_name = book_name.replace('>', '-')
        book_name = book_name.replace('<', '-')
        book_name = book_name.replace('|', '-')
        # Custom file name
        book_name = book_name.replace(' ', '-')
        book_name = book_name.replace('\'', '')
        book_name = book_name.replace(',', '')

        # Write .epub file
        cachesize = 1024
        with open('%s/%s.epub' %(file_dir, book_name), 'wb') as write_file:
            for chunk in res.iter_content(cachesize):
                if len(chunk) % cachesize != 0:
                    chunk += b' ' * ( cachesize - len(chunk))
                write_file.write(chunk)

        # Employ random delay
        time.sleep(3+random.randint(0,5))
{% endhighlight %}

### 爬蟲程式小結
寫網路爬蟲還是滿有趣的一件事，可能是很少實際寫一個完整的爬蟲，所以能在新鮮感還沒退去前先把程式寫好。
而且小弟在每一個下載行為之後都會刻意隨機延遲3~8秒，自以為模擬人類瀏覽行為(哈:D)。
這隻爬蟲純粹使用`requests`跟`BeautifulSoup`這兩個套件，極力避免使用`Selenium`。
原因是在於最後者會控制到網頁瀏覽器，若非真的遇到無解的動態網頁才會使用，若只使用前兩者來對取得網站資料，只需在console介面下執行，相較之下速度快上不少。

這個爬蟲還是有缺點尚待改進，程式每次都是由第一頁開始抓取，若是遇到**非預期程式中斷，則大俠就要重新來過**。
為了解決此問題，我額外設計了一個變數`last_item`，當`last_item=None`時，一如往常地從第一頁開始爬起；但若`last_item`為任意書名，程式將直接略到那本書的頁面，並從那本書開始下載，
詳見[**最終版本**](https://github.com/KodeWorker/NovelAlchemist/blob/master/web_scraping/novel_scraper.py)。

統計數據：
- 文件數量：
- 爬蟲時間：
- 檔案大小：

## 文檔正規化
(以下待續...)
