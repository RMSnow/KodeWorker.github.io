---
layout: single
title: 科幻小說鍊成計畫
date: 2017-07-21 18:52:31
excerpt: 紀錄我用程式自動生成科幻小說的學習歷程。
categories:
- 計畫
tags:
- procedural generation
- NaNoWriMo
- NaNoGenMo
- web scraping
- LSTM
- GAN
---

{% include toc title = "目錄" %}

## 前言

十一月越來越逼進了，那是一年一度的[國家小說寫作月](http://nanowrimo.org/)(National Novel Writing Month)了，身為文藝愛好者的我，怎麼可能錯過這場盛事呢？
雖然還有好一段時間，但是現在不好好開始規劃、慢慢利用零碎的時間持之以恆的學習，應該很難產生出有趣的成果。
在年初時候，我對遊戲相關的演算法非常感興趣，當時實作了尋找路徑的A*演算法、以及取得視野的ray-casting，接著慢慢地把興趣轉向了程序化生成(procedural generation)。
在網路上找到了一篇滿有趣的[文章](https://mewo2.com/notes/naming-language/)，作者是由奇幻小說的地名為發想，想要創造一組虛構的語言來對小說中的世界進行命名，文章中一步步從語言學的角度，由音節、字符逐步構建簡單的語言。
仔細看了一下文章開頭，天那！竟然是為了[NaNoGenMo](https://nanogenmo.github.io/)，國家小說產生月！
在寒冬時節，想像屋外冰天雪地，愜意地坐在火爐邊寫小說、讀小說，外加一杯香濃的熱可可，真是人生一大享受。
但是這時一群腦洞大開的程式員，不畏寒冬守在螢幕前敲著鍵盤，打算用程式碼來拯救全世界。

>「你用筆來寫小說？」
>
>「太遜了，我們都用程式來生成小說」

引起我極大興趣的另一個原因是，去年年底在reddit上讀到了關於Machine Learning conference的報導，其中Generative Adversarial Networks(GAN)受到高度的評價，被譽為是近十年來最棒的演算法。
最近台大[李宏毅教授](http://speech.ee.ntu.edu.tw/~tlkagk/index.html)也來到中心演講也是在介紹GAN，其中就有唐詩鍊成的範例，雖然語句不太有意義，但我認為是一個很好的參考指標。
接著是清大[張俊盛教授](http://www.nlplab.cc/)來中心介紹自然語言處理，這個課程講得概念很粗略，反而是老師為自然語言、機器翻譯畫了一個美好大餅，希望吸引博士生加入一起努力(讀博士？算了吧！)。
不過重點有提到，目前主流使用的Neural Machine Translation，由於忽略的語句的文法構成，仍有許多值得改進的地方。
聽完這一系列的演講，內心的熱血已在蠢蠢欲動，所以想利用NaNoGenMo這個性質比較輕鬆的活動，慢慢從頭規劃程式並產生至少能讀的科幻小說。

## 專案規劃
在施放鍊金術前，總是要有基本素材才能進行鍊成，直接無中生有已經不是鍊金術了，那已經是另一個次元位面的事情了。
大致上先將目前計畫規劃為下列四大部分，再逐一慢慢完成每個區塊。

**專案區塊**：
1. 網路爬蟲：收集已在公共領域、不會侵權的小說文章。
2. 文檔正規化：文檔資料前處理。
3. 構建文句產生器：參考[這篇文章](http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)，利用LSTM自動產生文句。
4. 利用GAN鍊出最佳文章

先建立一個Github Project: [小說煉金術師](https://github.com/KodeWorker/NovelAlchemist)來存放我的工作進度。

## 網路爬蟲
### 網站資源
為了要生成科幻小說，想當然耳要先去找尋相關類型的文章，有礙於著作權法，所以只能找尋已經開放於公共領域(public domain)的小說。
在網路上搜索相關資訊的過程中，發現了一篇介紹[25個存放公共領域書籍的免費網站](https://ebookfriendly.com/free-public-domain-books-sources/)，裡面有很完整的介紹。
其中最大的網站，[Project Gutenberg](http://www.gutenberg.org/)，是我心目中收集鍊金素材的最佳網站，但是很不幸它的使用條款其中一條寫道：

> **The Project Gutenberg website is for human users only.** Any real or perceived use of automated tools to access our site will result in a block of your IP address. This site utilizes cookies, captchas and related technologies to help assure the site is maximally available for human users only.

雖然之前看的[書本](https://automatetheboringstuff.com/chapter11/)裡，介紹到Web Scraping也是從Project Gutenberg下載羅密歐與茱麗葉，但是身為腦包研究者一定先把25個網站一個個看過，再決定我們的行動方針。
在排名第6的[Feedbooks](http://www.feedbooks.com/publicdomain)也是有非常多的公共領域書籍，其中科幻類的就有1176本！但是看到使用條例，心也涼了一半，不過條款的力道似乎沒有像Project Gutenberg那麼嚴厲。

> **6.15** use any robot, spider, scraper, or other automated means to access the FeedBooks Website bypass any measures FeedBooks may use to prevent or restrict access to the FeedBooks Website

小王子曾說過：『沙漠之所以這麼美，是因為在某個角落裡，藏著一口井。星星好美，是因為一朵我們看不見的花。』
現在終於讓我找到了，那個藏在沙漠角落的那一口井了，那就是[Manybooks](http://manybooks.net/)，這個網站雖然也收集了Project Gutenberg及其他網路資源，但是使用條款中完全沒有限制網路爬蟲，簡直是佛心公司。

### 爬蟲程式撰寫
整體的主程式架構說明如下，概念是先取得網站上所有的小說類別，接著再搜尋相關語言選項。
由於個人就是偏好科幻小說，而這個計畫也叫做"科幻"小說鍊成計畫，當然挑選Science Fiction。
在語言選擇方面，由於網路上大部分的資料是英語，為了方便資料處理與模型建置，首選English。
接著一頁一頁的爬過所有科幻類的頁面，每一頁開始尋找所有書籍的連結，並將所有書籍的`.epub`檔案下載(因為有些書籍只有`.epub`檔案)。
最後搜尋下個頁面的按鈕，若無按鈕則結束程式。

**程式架構**：
- 連接書籍網站(http://manybooks.net/)
- 取得所有類別(Genre)，並加以選取(預設Sci-Fi)
- 取得所有語言(Language)，並加以選取(預設English)
- 連結頁面設定為：相關類別、語言總集的第一頁
    1. 連結頁面
    2. 抓取頁面內所有書籍(若頁面內沒有書籍則結束)
    3. 取得下一個頁面連結(若無下一頁按鈕則結束)
    4. To `step. 1`

**主程式**
{% highlight python %}
    if __name__ == '__main__':

        # Settings
        file_dir = os.path.join(os.path.dirname(__file__),'epub')
        url = 'manybooks.net'

        # Create the folder to save scraped files
        if not os.path.exists(file_dir):
            os.makedirs(file_dir)

        # select=54 : Science Fiction
        genre_link = sel_genre(url, select=54)
        # select=10 : English
        lang = sel_language(genre_link, select=10)
        # First page of the selected genre
        page_link = genre_link + '/' + lang

        while True:

            # Access genre pages
            print('access... %s' %page_link)
            res = requests.get(page_link)
            try:
                res.raise_for_status()
            except Exception as exc:
                print('There was a problem accessing genre page: %s' %(exc))
            soup = bs4.BeautifulSoup(res.text)

            try:
                # Find books and download .epub files
                download_books_in_page(url, soup, file_dir)

                # Get "next page" button
                button_elems = soup.select('a[title="next"]')[0]
                page_link = 'http://%s%s' %(url, button_elems.get('href'))
            except Exception as e:
                # No "next page" button
                print(e)
                print('end of pages')
                break
{% endhighlight %}

首先我們要先取得所有小說的類型，如此一來方便我們未來的使用，可以挑選其他非科幻題材來鍊成文章。
{% highlight python %}
    def sel_genre(url='manybooks.net', select=None):
        """ Select Genre
        This function allows user to select a particular genre from "manybooks.net"
        and it returns the link to the first page of selected genre.

        Parameters
        ----------
        url: string, default='manybooks.net'
            The URL of "Manybooks".
        select: int, default=None
            The number of selection. If the value is None, the console will display
            all the genres and requires an user input.

        Return
        ------
        link: string
            The URL of the first page of the selected genre.
        """
        genre_url = 'http://%s/%s' %(url, 'categories')
        genre_dict = {}
        genre_list = []

        # Access genre catagories
        print('access genres... %s' %genre_url)
        res = requests.get(genre_url)
        try:
            res.raise_for_status()
        except Exception as exc:
            print('There was a problem accessing genre catagories: %s' %(exc))
        soup = bs4.BeautifulSoup(res.text)

        # Get all genre elements
        elems = soup.select('a[class="larger"]')
        for elem in elems:
            genre_name = elem.getText()
            genre_link = 'http://%s%s' %(url, elem.get('href'))
            genre_dict[genre_name] = genre_link
            genre_list.append(genre_name)

        # Display Genre Selection
        if select == None:
            for i in range(len(genre_list)):
                print('[%d] : %s' %(i+1, genre_list[i]))
            select = input('Select the Genre (no.) >> ')

            if not (int(select) > 0 and int(select) < len(genre_list)+1):
                raise ValueError('Wrong selection number!')

        print('genre: %s' %genre_list[int(select)-1])

        link = genre_dict[genre_list[int(select)-1]]
        return link
{% endhighlight %}

接著要選取對應的語言。
{% highlight python %}
    def sel_language(url, select=None):
        """ Select Language
        This function allows user to select a particular language in the pages of
        each genre and it returns the link to the page of books with selected
        language.

        Parameters
        ----------
        url: string
            The URL of the first selected genre page.
        select: int, default=None
            The number of selection. If the value is None, the console will display
            all the language and requires an user input.

        Return
        ------
        link: string
            The URL of the first page of the selected genre and selected language.
        """
        lang_dict = {}
        lang_list = []

        # Access language options
        print('access language... %s' %url)
        res = requests.get(url)
        try:
            res.raise_for_status()
        except Exception as exc:
            print('There was a problem accessing language options: %s' %(exc))
        soup = bs4.BeautifulSoup(res.text)

        # Get all language option elements
        elems = soup.select('option')
        for elem in elems:
            lang_name = elem.getText()
            lang_value = elem.get('value')
            lang_dict[lang_name] = lang_value
            lang_list.append(lang_name)

        # Display language Selection
        if select == None:
            for i in range(len(lang_list)):
                print('[%d] : %s' %(i+1, lang_list[i]))
            select = input('Select the Genre (no.) >> ')

            if not (int(select) > 0 and int(select) < len(lang_list)+1):
                raise ValueError('Wrong selection number!')

        print('language: %s' %lang_list[int(select)-1])
        return lang_dict[lang_list[int(select)- 1]]
{% endhighlight %}

最後就是最重要，下載頁面的部份。首先是取得一整個頁面所有書籍的連結，每一頁約略有20本書。
{% highlight python %}
    def download_books_in_page(url, soup, file_dir):
        """Download Books in the Genre Pages
        This function downloads all the books in a genre page and raise the stop
        signal if there is no books in the page.

        Parameters
        ----------
        url: string
            The root URL address.
        soup: bs4.BeautifulSoup object
            The object contains all the info. in the genre catagory page.
        file_dir: sting
            This is the folder to save the scraped files.
        """
        try:
            # has books
            # Get book elements in the genre page
            book_elems = soup.select('.grid_6.smallBook a')
            book_links =  [elem.get('href') for elem in book_elems]
            book_names = [elem.getText() for elem in book_elems]
            for i in range(len(book_links)):
                # Download one book at a time
                download_single_book(url, book_links[i], file_dir, book_names[i])
        except:
            # no book
            raise ScraperError('Error: No books in this page!')
{% endhighlight %}

類別頁面沒有書的時候，發出的以下自定義的Error。
{% highlight python %}
    class ScraperError(Exception):
        """ Scraper Error
        A custom error raised when there are troubles.
        """
        def __init__(self, value):
            self.value = value
        def __str__(self):
            return repr(self.value)
{% endhighlight %}

最最最關鍵，下載書籍連結的部分如下。
{% highlight python %}
    def download_single_book(url, book_link, file_dir, book_name):
        """Download a single Book
        This function download the particular book according to its book_link. We
        use URL (see -> # Text file download link) instead of using selenium module
        for better program control.

        Parameters
        ----------
        url: string
            The root URL address.
        book_link: string
            The href of the book.
        file_dir: string
            This is the folder to save the scraped files.
        book_name
            This is the name(filename) of the book.
        """
        # Text file download link
        book_id = book_link[8:-5]
        download_page = 'http://' + url + '/download-ebook?tid=' + book_id + \
        '&book=1%3Aepub%3A.epub%3Aepub'

        # Access the download page
        print('-> access... %s' %(book_name))
        res = requests.get(download_page)
        try:
            res.raise_for_status()
        except Exception as exc:
            print('There was a problem accessing download page: %s' %(exc))
        soup = bs4.BeautifulSoup(res.text)

        # Get downloadable elements
        download_elems = soup.select('a[rel="nofollow"]')
        epub_link = [elem.get('href') for elem in download_elems if \
                    elem.get('href').endswith('.epub')][0]

        # Download .epub file
        download_link = 'http://%s%s' %(url, epub_link)
        print('->-> access download link... %s' %(download_link))
        res = requests.get(download_link, allow_redirects=True)  # redirected!
        try:
            res.raise_for_status()
        except Exception as exc:
            print('There was a problem accessing download link: %s' %(exc))
        print('->-> download... %s' %(book_name))

        # Avoid file name errors
        book_name = book_name.replace('/', '-')
        book_name = book_name.replace('\\', '-')
        book_name = book_name.replace(':', '-')
        book_name = book_name.replace('*', '-')
        book_name = book_name.replace('?', '-')
        book_name = book_name.replace('\"', '-')
        book_name = book_name.replace('>', '-')
        book_name = book_name.replace('<', '-')
        book_name = book_name.replace('|', '-')
        # Custom file name
        book_name = book_name.replace(' ', '-')
        book_name = book_name.replace('\'', '')
        book_name = book_name.replace(',', '')

        # Write .epub file
        cachesize = 1024
        with open('%s/%s.epub' %(file_dir, book_name), 'wb') as write_file:
            for chunk in res.iter_content(cachesize):
                if len(chunk) % cachesize != 0:
                    chunk += b' ' * ( cachesize - len(chunk))
                write_file.write(chunk)

        # Employ random delay
        time.sleep(3+random.randint(0,5))
{% endhighlight %}

### 爬蟲程式小結
寫網路爬蟲還是滿有趣的一件事，可能是很少實際寫一個完整的爬蟲，所以能在新鮮感還沒退去前先把程式寫好。
而且小弟在每一個下載行為之後都會刻意隨機延遲3~8秒，自以為模擬人類瀏覽行為(哈:D)。
這隻爬蟲純粹使用`requests`跟`BeautifulSoup`這兩個套件，極力避免使用`Selenium`。
原因是在於最後者會控制到網頁瀏覽器，若非真的遇到無解的動態網頁才會使用，若只使用前兩者來對取得網站資料，只需在console介面下執行，相較之下速度快上不少。

這個爬蟲還是有缺點尚待改進，程式每次都是由第一頁開始抓取，若是遇到**非預期程式中斷，則大俠請重新來過**。
為了解決此問題，最後額外設計了一個變數`last_item`，當`last_item=None`時，一如往常地從第一頁開始爬起；但若`last_item`為任意書名，程式將直接略到那本書的頁面，並從那本書開始下載。

**統計數據**：
- 文件數量：1324
- 爬蟲時間：09:56:42
- 檔案大小：252 MB

網頁上顯示全部的英文科幻小說有1340筆，但是下載下來的`.epub`檔案僅有1324筆差距16筆，差距的資料應當寫一支程式來進行比對。
雖然差距的資料不多，但是本人身為一個低能好奇寶寶，還是得找出其中差異的原因。
最後測試結果得知，原來是有**相同名稱**的書本，所以爬蟲又要再做修改，爬蟲程式也有可能要重跑了...

詳見[最終版本](https://github.com/KodeWorker/NovelAlchemist/blob/master/web_scraping/novel_scraper.py)。

**最終版統計數據**：
- 文件數量：1340
- 爬蟲時間：09:21:38
- 檔案大小：265.7 MB

## 文檔正規化
爬蟲爬了好久竟然連一半的文章都還沒抓齊，與其看著reddit或PTT的文章顆顆笑，還不如接下來的準備工作。
文檔的正規化好比於資料的前置處理，依照之前研究的經驗，這部分應該會花費掉最多的功夫，預想大致的流程如下。
1. 首先，將抓下來的`.epub`文檔轉換成純文字的`.txt`文檔，可能會遭遇到檔案格式毀損、或是內文包含異常字符。
2. 接著將處理過後的`.txt`文檔拆解為兩個部分，一個是出版及檔案格式相關的`metadata`，另一個是純粹內文`content`。
3. 最後處理`content`部分應該最為複雜，`content`內可能包含非內容的目錄、章節標記、分隔符號，要規劃一個有系統的儲存格式方便後續模型建置。

### 檔案格式轉換
上網查詢了一下`.epub`轉`.txt`相關的程式，就這個[ebooklib](https://github.com/aerkalov/ebooklib)的程式最符合需求，而且大部分所需的package在Anaconda內都有了。
首先先fork到自己的Github，接著在專案中引入`ebooklib`，並且將`ebooklib/epub.py`中`read_file`副程式中的路徑comment起來(line 1298)。
這裡又學到了一招git submodule的專案管理方法(幹code、修code、用code)，應用時機是在專案下用到其他專案，同時要維護兩個reposiroty的獨立版本。
從同仁那裏詢問到大致的使用方法，再加上完整的[網路資料](https://git-scm.com/book/en/v2/Git-Tools-Submodules)說明，輕鬆地加入了ebooklib。

Git submodules重點如下：
1. 先fork原本的reposiroty方便進行修改
2. 在要引入的傳案內使用`git submodule add YOUR_FORKED_REPO_URL`
3. 利用`git submodule update --remote`將submodule更新到最新的版本
4. clone時使用`git clone --recursive YOUR_MAIN_PROJECT`

雖然另一位同仁指出可以利用[Bitbucket](https://bitbucket.org/)，將整個repository匯入到獨立的地方控管，但是這個做法跟Github上的fork根本一樣，為了方便個人管理還是使用fork。

以下副程式利用了`ebooklib`，將單一檔案進行轉換。
{% highlight python %}
    def convert_epub_to_txt(epub_path, txt_path, encode='utf-8'):
        """ Convert EPUB to TXT
        This function converts the craped .epub files to .txt files.

        Parameters
        ----------
        epub_path: string
            This is the path of a craped .epub file to be parsed.
        txt_path: string
            This is the path of the converted .txt file to be saved.
        encode: string, default='utf-8'
            The encoding method of the character set.
        """
        try:
            book = epub.read_epub(epub_path)        
            content = ''        
            # Get all html content in .epub
            for doc in  book.get_items_of_type(ebooklib.ITEM_DOCUMENT):
                content = str('%s%s') %(content, doc.content)

            # Select the text part
            soup = bs4.BeautifulSoup(content)
            elems = soup.findAll(['h1', 'h2', 'h3', 'h4', 'p', 'pre'])
            lines = [(elem.getText() + '\n') for elem in elems]
        except Exception as e:
            # BadZipFile -> parse an empty .txt file
            lines = []
            print(e)

        # Write .txt file
        with open(txt_path, 'wb') as write_file:
            write_file.writelines([line.encode(encode) for line in lines])
{% endhighlight %}

不過解析`.epub`檔案，還是利用`BeautifulSoup`來處理網頁格式的檔案比較方便，由於只`h1`, `h2`, `h3`, `h4`, `p`, `pre`這些區塊的文字，也可能會有遺珠的可能性。

### 檔案大小分析
在進行更進一步的文檔正規劃之前，我們需要先對格式轉化後的資料進行分析，以便初步剃除毀損或不全的資料。
大概先以人眼分析得出`.txt`檔案大小大概分三類，第一類`0 KB`標準的毀天滅地`.epub`壓縮檔毀損；第二類約為`1 KB`有兩種可能，第一是檔案裡面就是沒文檔，如"1/2986"，第二是檔案結構異於常人、骨骼驚奇，如"After the Cure"；最後第三類就是大部分正常轉出的文檔。
{% highlight python %}
    def analyze_parsed_text(txt_dir, result_dir, threshold):
        """ Analyze Parsed Text
        This function will generate statistics of parsed .txt files.

        Parameters
        ----------
        txt_dir: string
            This is the directory of the parsed .txt files
        fig_dir: string
            This is the directory to save the figuares of analyzed results
        threshold: float
            This is file size (Kb) threshold. The files with size smaller than the
            threshold will be considered as abnormal data.

        Return
        ------
        anomalies_name: list of strings
            This list contains names of all the abnormal files.
        """

        # Logging Settings
        log_name = os.path.join(result_dir, 'parsed_file_stats.log')
        logger = logging.getLogger('analyze_parsed_text')
        logger.setLevel(logging.DEBUG)

        # Delete all the handlers in the logger
        logger.handlers = []

        # Set the mode 'a'(apend) -> 'w'(overwrite)
        file_handler = logging.FileHandler(log_name, mode='w')
        file_handler.setLevel(logging.DEBUG)

        # Set logging format
        formatter = logging.Formatter('%(message)s')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

        # Read parsed .txt files
        txt_files = [x[2] for x in os.walk(txt_dir)][0]
        file_size = []
        for txt_file in txt_files:
            file_size.append(os.path.getsize(os.path.join(txt_dir, txt_file)) \
                             / 1000) # Unit in KB

        # Sort the file by it's size
        sorted_file = [x for (y, x) in sorted(zip(file_size, txt_files))]
        sorted_size = sorted(file_size)

        # Get abnormal files
        anomalies_name = [sorted_file[i] for i in range(len(sorted_size)) \
                          if sorted_size[i] < threshold]
        anomalies_size = [x for x in sorted_size if x < threshold]

        # Print abnormal files
        logger.info('Anomalies:')
        for i in range(len(anomalies_name)):
            logger.info('- %s (%.2f KB)' %(anomalies_name[i], anomalies_size[i]))
        logger.info('Radio: %d / %d (%.4f %%)' %(len(anomalies_name), \
                    len(txt_files), len(anomalies_name) / len(txt_files) * 100))        

        # The minial-sized normal file for checking
        logger.info('-- the minial-sized normal file: %s (%.2f KB) --' %( \
              sorted_file[len(anomalies_name)], sorted_size[len(anomalies_name)]))

        # Plot histogram
        # Figure settings    
        fig_dir = os.path.join(result_dir, 'fig')
        if not os.path.exists(fig_dir):
            os.makedirs(fig_dir)

        fig_size = (15, 5)
        bin_interval = 10
        dpi = 200

        plt.figure(figsize=fig_size)
        plt.title('Parsed .txt File Statistics')
        plt.xlabel('File size (KB)')
        plt.ylabel('Number of Files')
        bins = int(max(file_size)/bin_interval)    
        plt.hist(file_size, bins=bins)
        plt.savefig(os.path.join(fig_dir, 'parsed_file_stats.jpg'), dpi=dpi)
        plt.close()

        return anomalies_name
{% endhighlight %}

對檔案大小進行分析是為了確定，毀損或異常的檔案占整體的比重。
若比例很小，我們則可以大膽的直接捨棄這些異常文檔，但比例若太大，我們則需設計例外處理，取得其他檔案格式或是載點。
目前所有轉出的檔案，檔案大小分析數據及圖形如下。
解析後的檔案大小若小於`1 KB`，我們則認定為檔案異常，並且以`logging`紀錄異常，並記錄最小正常的檔案以供檢驗。

![解析資料大小分析](/assets/images/計畫/科幻小說鍊成計畫/parsed_file_stats.jpg){: .align-center}

**統計數據**
- 正常文檔數量：1340
- 異常文檔數量：13
- 異常文檔比例：0.9701 %

**遭過濾文檔**
- Rip-Foster-in-Ride-the-Gray-Planet.txt (0.00 KB)
- The-Holes-Around-Mars.txt (0.00 KB)
- 1-2986.txt (0.05 KB)
- After-The-Ending.txt (0.05 KB)
- After-the-Cure.txt (0.05 KB)
- Citadel.txt (0.05 KB)
- Forager.txt (0.05 KB)
- Nano-Contestant.txt (0.05 KB)
- Open-Minds.txt (0.05 KB)
- Phantasia.txt (0.05 KB)
- The-Boy-Who-Lit-Up-the-Sky.txt (0.05 KB)
- The-Eden-Plague.txt (0.05 KB)
- Zero.txt (0.05 KB)


觀察解析過後最小的正常文檔`Longevity.txt (3.34 KB)`後，發現阿就真的這麼短，想要瞬間變30CM都優秀鄉民也是很困難的。
由於異常文檔所佔比例很低，所以決定直接將這13個檔案捨去，得到1327個`.txt`文檔。

### 文章解剖

(以下待續...)
